{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "textile-acoustic",
   "metadata": {},
   "source": [
    "![](images/gpulab.png)\n",
    "\n",
    "# PyTorch Image Classifier Demonstration\n",
    "\n",
    "The following [GPULab](https://gpulab.io) demonstration uses [PyTorch](https://pytorch.org/) to create and train a flower species image [classifier](https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623) using the [102 Category Flower Dataset](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html) available from Oxford's [Visual Geometry Group](https://www.robots.ox.ac.uk/~vgg/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-remains",
   "metadata": {},
   "source": [
    "Import all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "from IPython.display import Image as NBImage\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-greeting",
   "metadata": {},
   "source": [
    "Set a few default values and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/flowers/\"\n",
    "source_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/\"\n",
    "image_source = \"102flowers.tgz\"\n",
    "label_source = \"imagelabels.mat\"\n",
    "training_dir = 'training/'\n",
    "validation_dir = 'validation/'\n",
    "test_dir = 'test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-report",
   "metadata": {},
   "source": [
    "## Aquire Source Data\n",
    "\n",
    "Download data lables and if it does not exists using the [requests](https://requests.readthedocs.io/en/master/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data directory if it does not exist\n",
    "if os.path.isdir(data_dir) is False:\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# download source data if it does not exist\n",
    "for file in [image_source, label_source]:\n",
    "    if os.path.isfile(data_dir + file) is False:\n",
    "        print(f\"{data_dir + file} is not found, downloading...\")\n",
    "        r = requests.get(source_url + file)\n",
    "        with open(data_dir + file, 'wb') as f:\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-attention",
   "metadata": {},
   "source": [
    "Extract images from the downloaded [tarfile](https://docs.python.org/3/library/tarfile.html) fontaining 8,189 flower images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(data_dir + \"jpg/image_00001.jpg\") is False:\n",
    "    images_tar = tarfile.open(data_dir + image_source)\n",
    "    images_tar.extractall(data_dir)\n",
    "    images_tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-blame",
   "metadata": {},
   "source": [
    "## Sort Data\n",
    "\n",
    "Seperate data into training, validation and test sets, reate a directory for each if they do not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in [training_dir, validation_dir, test_dir]:\n",
    "    if os.path.isdir(data_dir + directory) is False:\n",
    "        os.mkdir(data_dir + directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-austria",
   "metadata": {},
   "source": [
    "This demonstration uses the [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) class from [torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html) to load images. Images must be organized in subdirectories naed as the category for the images it contains. \n",
    "\n",
    "Use [scipy.io](https://docs.scipy.org/doc/scipy/reference/io.html) to load the image labels from the downloaded MATLABÂ® file (.mat). Alternatively, use the GPULab Octave kernel to work directly with most MATLAB-style files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mat = sio.loadmat(data_dir + \"imagelabels.mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-minimum",
   "metadata": {},
   "source": [
    "Ensure the lables are accessable. The label array should contain 8189 elements representing a numberic category for each of the 8,189 flower images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels_mat['labels'][0])\n",
    "len(labels) == 8189"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-complaint",
   "metadata": {},
   "source": [
    "Create a random index for sorting files using the length of the label set. Split the random index list into three groups: ~70% into a training set, ~20% into a validation set and ~10% into a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_n = len(labels)\n",
    "image_index = np.arange(index_n)\n",
    "np.random.shuffle(image_index)\n",
    "\n",
    "t_n = round(index_n * .70)\n",
    "v_n = round(index_n * .20)\n",
    "\n",
    "# create a dictionary of directories and a range of label indexes\n",
    "idx_dict = {\n",
    "    training_dir: image_index[0:t_n],\n",
    "    validation_dir: image_index[t_n:t_n+v_n],\n",
    "    test_dir: image_index[t_n+v_n:],\n",
    "}\n",
    "\n",
    "# ensure the indexes add up to the total numer of lables\n",
    "(len(idx_dict[training_dir])\n",
    " + len(idx_dict[validation_dir])\n",
    " + len(idx_dict[test_dir])) == index_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-shirt",
   "metadata": {},
   "source": [
    "Move files to their proper folders.\n",
    "\n",
    "`labels` holds the category value of image number (starting at 1) to its index number - 1 (zero based index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in idx_dict:\n",
    "    for idx in idx_dict[d]:\n",
    "        file_name = f\"image_{idx+1:05d}.jpg\"\n",
    "        source_file = f\"{data_dir}jpg/{file_name}\"\n",
    "        dest_dir = f\"{data_dir}{d}{labels[idx]}\"\n",
    "        dest_file = f\"{dest_dir}/{file_name}\"\n",
    "        # print(f\"{source_file} -> {dest_file}\")\n",
    "        if os.path.isdir(dest_dir) is False:\n",
    "            os.mkdir(dest_dir)\n",
    "        if os.path.isfile(source_file) is True:\n",
    "            shutil.move(source_file, dest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-chrome",
   "metadata": {},
   "source": [
    "## Review Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = [\"pink primrose\", \"hard-leaved pocket orchid\",\n",
    "               \"canterbury bells\", \"sweet pea\", \"english marigold\",\n",
    "               \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\",\n",
    "               \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\",\n",
    "               \"spear thistle\", \"yellow iris\", \"globe-flower\", \n",
    "               \"purple coneflower\", \"peruvian lily\", \"balloon flower\",\n",
    "               \"giant white arum lily\", \"fire lily\", \"pincushion flower\",\n",
    "               \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\",\n",
    "               \"prince of wales feathers\", \"stemless gentian\", \"artichoke\",\n",
    "               \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\",\n",
    "               \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\",\n",
    "               \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\",\n",
    "               \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\",\n",
    "               \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\",\n",
    "               \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\",\n",
    "               \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\",\n",
    "               \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia\",\n",
    "               \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\",\n",
    "               \"silverbush\", \"californian poppy\", \"osteospermum\",\n",
    "               \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\",\n",
    "               \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\",\n",
    "               \"morning glory\", \"passion flower\", \"lotus lotus\", \"toad lily\",\n",
    "               \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\",\n",
    "               \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\",\n",
    "               \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\",\n",
    "               \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\",\n",
    "               \"mexican petunia\", \"bromelia\", \"blanket flower\",\n",
    "               \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "print(f\"text_labels holds {len(text_labels)} category names\"\n",
    "      \" ordered by the label id.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 6\n",
    "print(f\"Category {cat} is {text_labels[cat - 1]}:\")\n",
    "\n",
    "cat_dir = data_dir+training_dir+str(cat)+'/'\n",
    "random_file = random.choice(os.listdir(cat_dir))\n",
    "NBImage(filename=cat_dir+random_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-browse",
   "metadata": {},
   "source": [
    "## Load and Transform Data\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained network expectations\n",
    "# see: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "expected_means = [0.485, 0.456, 0.406]\n",
    "expected_std = [0.229, 0.224, 0.225]\n",
    "max_img_size = 224\n",
    "batch_size = 32\n",
    "\n",
    "# transforms for the training, validation, and testing sets\n",
    "tfx = {\n",
    "    \"training\": transforms.Compose([transforms.RandomHorizontalFlip(p=0.25),\n",
    "                                    transforms.RandomRotation(25),\n",
    "                                    transforms.RandomGrayscale(p=0.02),\n",
    "                                    transforms.RandomResizedCrop(max_img_size),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(expected_means,\n",
    "                                                         expected_std)]),\n",
    "\n",
    "    \"validation\": transforms.Compose([transforms.Resize(max_img_size + 1),\n",
    "                                      transforms.CenterCrop(max_img_size),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(expected_means,\n",
    "                                                           expected_std)]),\n",
    "\n",
    "    \"testing\": transforms.Compose([transforms.Resize(max_img_size + 1),\n",
    "                                   transforms.CenterCrop(max_img_size),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize(expected_means,\n",
    "                                                        expected_std)])\n",
    "\n",
    "}\n",
    "\n",
    "# load the datasets with ImageFolder class\n",
    "flower_datasets = {\n",
    "    \"training\": datasets.ImageFolder(data_dir+training_dir,\n",
    "                                     transform=tfx[\"training\"]),\n",
    "\n",
    "    \"validation\": datasets.ImageFolder(data_dir+validation_dir,\n",
    "                                       transform=tfx[\"validation\"]),\n",
    "\n",
    "    \"testing\": datasets.ImageFolder(data_dir+test_dir, \n",
    "                                    transform=tfx[\"testing\"])\n",
    "}\n",
    "\n",
    "# define the dataloaders using the image datasets and the trainforms\n",
    "dataloaders = {\n",
    "    \"training\": torch.utils.data.DataLoader(flower_datasets[\"training\"],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True),\n",
    "\n",
    "    \"validation\": torch.utils.data.DataLoader(flower_datasets[\"validation\"],\n",
    "                                              batch_size=batch_size),\n",
    "\n",
    "    \"testing\": torch.utils.data.DataLoader(flower_datasets[\"testing\"],\n",
    "                                           batch_size=batch_size)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-timing",
   "metadata": {},
   "source": [
    "## Construct Network\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model Output Size = Number of Categories\n",
    "output_size = len(text_labels)\n",
    "\n",
    "# Using VGG16.\n",
    "nn_model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Input size from current classifier\n",
    "input_size = nn_model.classifier[0].in_features\n",
    "hidden_size = [\n",
    "    (input_size // 8),\n",
    "    (input_size // 32)\n",
    "]\n",
    "\n",
    "# Prevent backpropigation on parameters\n",
    "for param in nn_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create nn.Module with Sequential using an OrderedDict\n",
    "# See https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "        ('fc1', nn.Linear(input_size, hidden_size[0])),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('dropout', nn.Dropout(p=0.15)),\n",
    "        ('fc2', nn.Linear(hidden_size[0], hidden_size[1])),\n",
    "        ('relu2', nn.ReLU()),\n",
    "        ('dropout', nn.Dropout(p=0.15)),\n",
    "        ('output', nn.Linear(hidden_size[1], output_size)),\n",
    "        # LogSoftmax is needed by NLLLoss criterion\n",
    "        ('softmax', nn.LogSoftmax(dim=1))\n",
    "    ]))\n",
    "\n",
    "# Replace classifier\n",
    "nn_model.classifier = classifier\n",
    "\n",
    "hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-testament",
   "metadata": {},
   "source": [
    "## Train Network\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# https://en.wikipedia.org/wiki/Hyperparameter\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "chk_every = 50\n",
    "\n",
    "# Start clean by setting gradients of all parameters to zero.\n",
    "nn_model.zero_grad()\n",
    "\n",
    "# The negative log likelihood loss as criterion.\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Adam: A Method for Stochastic Optimization\n",
    "# https://arxiv.org/abs/1412.6980\n",
    "optimizer = optim.Adam(nn_model.classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move model to perferred device.\n",
    "nn_model = nn_model.to(device)\n",
    "\n",
    "data_set_len = len(dataloaders[\"training\"].batch_sampler)\n",
    "total_val_images = len(dataloaders[\"validation\"].batch_sampler) * dataloaders[\"validation\"].batch_size\n",
    "\n",
    "print(f'Using the {device} device to train.')\n",
    "\n",
    "print(f'Training on {data_set_len} batches '\n",
    "      f'of {dataloaders[\"training\"].batch_size}.')\n",
    "\n",
    "print('Displaying average loss and accuracy for '\n",
    "      f'epoch every {chk_every} batches.')\n",
    "\n",
    "for e in range(epochs):\n",
    "    e_loss = 0\n",
    "    prev_chk = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    print(f'\\nEpoch {e+1} of {epochs}\\n----------------------------')\n",
    "    for ii, (images, labels) in enumerate(dataloaders[\"training\"]):\n",
    "        # Move images and labeles preferred device\n",
    "        # if they are not already there\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Set gradients of all parameters to zero.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Propigate forward and backward\n",
    "        outputs = nn_model.forward(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keep a running total of loss for\n",
    "        # this epoch\n",
    "        e_loss += loss.item()\n",
    "\n",
    "        # Accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Keep a running total of loss for\n",
    "        # this epoch\n",
    "        itr = (ii + 1)\n",
    "        if itr % chk_every == 0:\n",
    "            avg_loss = f'avg. loss: {e_loss/itr:.4f}'\n",
    "            acc = f'accuracy: {(correct/total) * 100:.2f}%'\n",
    "            print(f'  Batches {prev_chk:03} to {itr:03}: {avg_loss}, {acc}.')\n",
    "            prev_chk = (ii + 1)\n",
    "\n",
    "    # Validate Epoch\n",
    "    e_valid_correct = 0\n",
    "    e_valid_total = 0\n",
    "\n",
    "    # Disabling gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for ii, (images, labels) in enumerate(dataloaders[\"validation\"]):\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = nn_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            e_valid_total += labels.size(0)\n",
    "            e_valid_correct += (predicted == labels).sum().item()\n",
    "        print(f\"\\n\\tValidating for epoch {e+1}...\")\n",
    "        correct_perc = 0\n",
    "        if e_valid_correct > 0:\n",
    "            correct_perc = (100 * e_valid_correct // e_valid_total)\n",
    "        print(f'\\tAccurately classified {correct_perc:d}% of {total_val_images} images.')\n",
    "\n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-price",
   "metadata": {},
   "source": [
    "## Test Network\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model validation with the test dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "total_images = len(dataloaders[\"testing\"].batch_sampler) * dataloaders[\"testing\"].batch_size\n",
    "\n",
    "# Disabling gradient calculation\n",
    "with torch.no_grad():\n",
    "    for ii, (images, labels) in enumerate(dataloaders[\"testing\"]):\n",
    "        # Move images and labeles to perferred device\n",
    "        # if they are not already there\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = nn_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accurately classified {(100 * correct // total):d}% of {total_images} images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-istanbul",
   "metadata": {},
   "source": [
    "## Save Model State\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.class_to_idx = flower_datasets['training'].class_to_idx\n",
    "\n",
    "model_state = {\n",
    "    'epoch': epochs,\n",
    "    'state_dict': nn_model.state_dict(),\n",
    "    'optimizer_dict': optimizer.state_dict(),\n",
    "    'classifier': classifier,\n",
    "    'class_to_idx': nn_model.class_to_idx,\n",
    "}\n",
    "\n",
    "torch.save(model_state, 'model_state.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-hawaiian",
   "metadata": {},
   "source": [
    "## Loading Saved Model State\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load a saved model state and rebuild the model\n",
    "\n",
    "def load_model(file='model_state.pth'):\n",
    "    # Loading weights for CPU model while trained on GP\n",
    "    # https://discuss.pytorch.org/t/loading-weights-for-cpu-model-while-trained-on-gpu/1032\n",
    "    model_state = torch.load(file, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    model.classifier = model_state['classifier']\n",
    "    model.load_state_dict(model_state['state_dict'])\n",
    "    model.class_to_idx = model_state['class_to_idx']\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "saved_model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-sterling",
   "metadata": {},
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    expects_means = [0.485, 0.456, 0.406]\n",
    "    expects_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    pil_image = Image.open(image).convert(\"RGB\")\n",
    "\n",
    "    # Any reason not to let transforms do all the work here?\n",
    "    in_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                        transforms.CenterCrop(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(expects_means, expects_std)])\n",
    "    pil_image = in_transforms(pil_image)\n",
    "\n",
    "    return pil_image\n",
    "\n",
    "\n",
    "rose_dir = data_dir+training_dir+'/74/'\n",
    "\n",
    "random_rose = random.choice(os.listdir(rose_dir))\n",
    "\n",
    "# process a PIL image for use in a PyTorch model\n",
    "rose_image = process_image(rose_dir+random_rose)\n",
    "type(rose_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.transpose((1, 2, 0))\n",
    "\n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "\n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "imshow(rose_image.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-cooperative",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model, topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "\n",
    "    # evaluation mode\n",
    "    # https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval\n",
    "    model.eval()\n",
    "\n",
    "    # cpu mode\n",
    "    model.cpu()\n",
    "\n",
    "    # load image as torch.Tensor\n",
    "    image = process_image(image_path)\n",
    "\n",
    "    # Unsqueeze returns a new tensor with a dimension of size one\n",
    "    # https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Disabling gradient calculation \n",
    "    # (not needed with evaluation mode?)\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(image)\n",
    "        top_prob, top_labels = torch.topk(output, topk)\n",
    "\n",
    "        # Calculate the exponentials\n",
    "        top_prob = top_prob.exp()\n",
    "\n",
    "    class_to_idx_inv = {model.class_to_idx[k]: k for k in model.class_to_idx}\n",
    "    mapped_classes = list()\n",
    "\n",
    "    for label in top_labels.numpy()[0]:\n",
    "        mapped_classes.append(class_to_idx_inv[label])\n",
    "\n",
    "    return top_prob.numpy()[0], mapped_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image along with the top 5 classes\n",
    "\n",
    "chk_image_file = rose_dir+random_rose\n",
    "correct_class = text_labels[74 - 1]\n",
    "\n",
    "top_prob, top_classes = predict(chk_image_file, saved_model)\n",
    "\n",
    "top_category = top_classes[0]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "sp_img = plt.subplot2grid((15, 9), (0, 0), colspan=9, rowspan=9)\n",
    "sp_prd = plt.subplot2grid((15, 9), (9, 2), colspan=5, rowspan=5)\n",
    "\n",
    "image = Image.open(chk_image_file)\n",
    "sp_img.axis('off')\n",
    "sp_img.set_title(f'{text_labels[int(top_category) - 1]}')\n",
    "sp_img.imshow(image)\n",
    "\n",
    "labels = []\n",
    "for class_idx in top_classes:\n",
    "    labels.append(text_labels[int(class_idx) - 1])\n",
    "\n",
    "yp = np.arange(5)\n",
    "sp_prd.set_yticks(yp)\n",
    "sp_prd.set_yticklabels(labels)\n",
    "sp_prd.set_xlabel('Probability')\n",
    "sp_prd.invert_yaxis()\n",
    "sp_prd.barh(yp, top_prob, xerr=0, align='center', color='blue')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Correct classification: {correct_class}')\n",
    "print('Correct prediction: '\n",
    "      f'{correct_class == text_labels[int(top_category) - 1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-michigan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
